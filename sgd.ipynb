{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load table into pandas df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "ratings_df = pd.read_csv('sample_data/ratings.csv')\n",
    "\n",
    "print(ratings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train (70%), dev (15%), test (15%)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(ratings_df, test_size=0.3, random_state=42)\n",
    "dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-level explanation of SGD: \n",
    "1. We initialize a user embedding table and an item embedding table\n",
    "2. We use DataLoader to set up our data. \n",
    "3. Forward pass\n",
    "     Let's say user of id 1 rated movie of id 4.\n",
    "     We look up the user embedding of userid=1 and the item embedding of itemid=4.\n",
    "     We perform element-wise multiplication of the two embeddings and sum the result\n",
    "     This is the predicted rating\n",
    "4. Backward pass\n",
    "     We compare the predicted rating with the label rating using MSE.\n",
    "     Update the user and item embedding tables accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model: we create user and item embedding tables for each unique user and unique item; \n",
    "# forward pass by element-wise multiplication and summation\n",
    "\n",
    "class MatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        # create user embeddings: each user is a tensor of length n_factors.\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors)\n",
    "        # create item embeddings: each item is a tensor of length n_factors\n",
    "        self.item_factors = torch.nn.Embedding(n_items, n_factors)\n",
    "        self.user_factors.weight.data.uniform_(0, 0.05)\n",
    "        self.item_factors.weight.data.uniform_(0, 0.05)\n",
    "\n",
    "    def forward(self, data):\n",
    "        users, items = data[:,0], data[:,1]\n",
    "        return (self.user_factors(users) * self.item_factors(items)).sum(1)\n",
    "\n",
    "    def predict(self, user, item):\n",
    "        return (self.user_factors(user) * self.item_factors(item)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need this to load the ratings pandas df into torch tensors. \n",
    "\n",
    "class Loader(Dataset):\n",
    "    def __init__(self, ratings_df):\n",
    "        self.ratings = ratings_df.copy()\n",
    "        users = ratings_df.userId.unique()\n",
    "        movies = ratings_df.movieId.unique()\n",
    "\n",
    "        self.userid2idx = {o:i for i,o in enumerate(users)}\n",
    "        self.movieid2idx = {o:i for i,o in enumerate(movies)}\n",
    "\n",
    "        self.idx2userid = {i:o for o,i in self.userid2idx.items()}\n",
    "        self.idx2movieid = {i:o for o,i in self.movieid2idx.items()}\n",
    "\n",
    "        self.ratings.movieId = ratings_df.movieId.apply(lambda x: self.movieid2idx[x])\n",
    "        self.ratings.userId = ratings_df.userId.apply(lambda x: self.userid2idx[x])\n",
    "\n",
    "        # userid and corresponding movieid that they rated\n",
    "        self.x = self.ratings.drop(['rating', 'timestamp'], axis=1).values\n",
    "        # rating\n",
    "        self.y = self.ratings['rating'].values\n",
    "        self.x, self.y = torch.tensor(self.x), torch.tensor(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_dev(train_df, dev_df, sgd_param):\n",
    "  \n",
    "  n_users = ratings_df['userId'].nunique()\n",
    "  n_items = ratings_df['movieId'].nunique()\n",
    "\n",
    "  model = MatrixFactorization(n_users, n_items, n_factors=sgd_param['n_factors'])\n",
    "  optimizer = optim.SGD(model.parameters(), lr=sgd_param['lr'])\n",
    "\n",
    "  cuda = torch.cuda.is_available()\n",
    "\n",
    "  if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "  loss_fn = nn.MSELoss()\n",
    "  train_set = Loader(train_df)\n",
    "  # This splits dataset into batches\n",
    "  train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for epoch in range(sgd_param['epochs']):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        # calculate loss\n",
    "        loss = loss_fn(output, target.float())\n",
    "        # calculate gradient\n",
    "        loss.backward()\n",
    "        # update parameters (embeddings in this case)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "  dev_set = Loader(dev_df)\n",
    "  dev_loader = DataLoader(dev_set, batch_size=128, shuffle=False)\n",
    "  # model to eval mode\n",
    "  model.eval()\n",
    "  dev_preds, dev_targets = [], []\n",
    "  with torch.no_grad():\n",
    "      for data, target in dev_loader:\n",
    "          data, target = data.cuda(), target.cuda()\n",
    "          output = model(data)\n",
    "          # append results\n",
    "          dev_preds.extend(output.cpu().numpy())\n",
    "          dev_targets.extend(target.cpu().numpy())\n",
    "\n",
    "  mae = mean_absolute_error(dev_targets, dev_preds)\n",
    "\n",
    "  print(f\"dev_mae for {sgd_param['n_factors']} factors, {sgd_param['lr']} lr, {sgd_param['epochs']} epochs: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_f in [10, 20, 50, 100]:\n",
    "  for n_ep in [10, 50, 100]:\n",
    "    for lr in [0.001, 0.01, 0.05]:\n",
    "      train_eval_dev(train_df, dev_df, {'n_factors': n_f, 'epochs': n_ep, 'lr': lr})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "dev_mae for 10 factors, 0.001 lr, 10 epochs: 3.4900412809292685\n",
    "dev_mae for 10 factors, 0.01 lr, 10 epochs: 3.480439918096354\n",
    "dev_mae for 10 factors, 0.05 lr, 10 epochs: 3.028467126331669\n",
    "dev_mae for 10 factors, 0.001 lr, 50 epochs: 3.486756268621376\n",
    "dev_mae for 10 factors, 0.01 lr, 50 epochs: 3.026092983281317\n",
    "dev_mae for 10 factors, 0.05 lr, 50 epochs: 1.3760415739293925\n",
    "dev_mae for 10 factors, 0.001 lr, 100 epochs: 3.480752333602614\n",
    "dev_mae for 10 factors, 0.01 lr, 100 epochs: 2.1650907983948624\n",
    "dev_mae for 10 factors, 0.05 lr, 100 epochs: 1.1044064729262975\n",
    "dev_mae for 20 factors, 0.001 lr, 10 epochs: 3.483121689989763\n",
    "dev_mae for 20 factors, 0.01 lr, 10 epochs: 3.4644898156541255\n",
    "dev_mae for 20 factors, 0.05 lr, 10 epochs: 2.8658078690881816\n",
    "dev_mae for 20 factors, 0.001 lr, 50 epochs: 3.476532301745354\n",
    "dev_mae for 20 factors, 0.01 lr, 50 epochs: 2.869004440047278\n",
    "dev_mae for 20 factors, 0.05 lr, 50 epochs: 1.3476828530524387\n",
    "dev_mae for 20 factors, 0.001 lr, 100 epochs: 3.4646104475111694\n",
    "dev_mae for 20 factors, 0.01 lr, 100 epochs: 2.0680492953973366\n",
    "dev_mae for 20 factors, 0.05 lr, 100 epochs: 1.0965616834833602\n",
    "dev_mae for 50 factors, 0.001 lr, 10 epochs: 3.4622471793977434\n",
    "dev_mae for 50 factors, 0.01 lr, 10 epochs: 3.4174553854509817\n",
    "dev_mae for 50 factors, 0.05 lr, 10 epochs: 2.6361976552181994\n",
    "dev_mae for 50 factors, 0.001 lr, 50 epochs: 3.445969564824434\n",
    "dev_mae for 50 factors, 0.01 lr, 50 epochs: 2.6354201712233962\n",
    "dev_mae for 50 factors, 0.05 lr, 50 epochs: 1.3040099250541246\n",
    "dev_mae for 50 factors, 0.001 lr, 100 epochs: 3.4169696260114346\n",
    "dev_mae for 50 factors, 0.01 lr, 100 epochs: 1.9297473189003211\n",
    "dev_mae for 50 factors, 0.05 lr, 100 epochs: 1.0835130303438045\n",
    "dev_mae for 100 factors, 0.001 lr, 10 epochs: 3.4275661815249725\n",
    "dev_mae for 100 factors, 0.01 lr, 10 epochs: 3.3410197442210903\n",
    "dev_mae for 100 factors, 0.05 lr, 10 epochs: 2.438167753674767\n",
    "dev_mae for 100 factors, 0.001 lr, 50 epochs: 3.396717071175329\n",
    "dev_mae for 100 factors, 0.01 lr, 50 epochs: 2.440149415787094\n",
    "dev_mae for 100 factors, 0.05 lr, 50 epochs: 1.2641956796488487\n",
    "dev_mae for 100 factors, 0.001 lr, 100 epochs: 3.341480087737653\n",
    "dev_mae for 100 factors, 0.01 lr, 100 epochs: 1.8127504039392\n",
    "dev_mae for 100 factors, 0.05 lr, 100 epochs: 1.0718658247723067 <- Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = ratings_df['userId'].nunique()\n",
    "n_items = ratings_df['movieId'].nunique()\n",
    "\n",
    "model = MatrixFactorization(n_users, n_items, n_factors=100)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "train_set = Loader(train_df)\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = (d.cuda() for d in data), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "test_set = Loader(test_df)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "test_preds, test_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = (d.cuda() for d in data), target.cuda()\n",
    "        output = model(data)\n",
    "        test_preds.extend(output.cpu().numpy())\n",
    "        test_targets.extend(target.cpu().numpy())\n",
    "\n",
    "mae = mean_absolute_error(test_targets, test_preds)\n",
    "mse = mean_squared_error(test_targets, test_preds)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
