{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load table into pandas df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "ratings_df = pd.read_csv('sample_data/ratings.csv')\n",
    "\n",
    "print(ratings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train (70%), dev (15%), test (15%)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(ratings_df, test_size=0.3, random_state=42)\n",
    "dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level explanation of SGD: \n",
    "# 1. We initialize a user embedding table and an item embedding table\n",
    "# 2. We use DataLoader to set up our data. \n",
    "# 3. Forward pass\n",
    "#      Let's say user of id 1 rated movie of id 4.\n",
    "#      We look up the user embedding of userid=1 and the item embedding of itemid=4.\n",
    "#      We perform element-wise multiplication of the two embeddings and sum the result\n",
    "#      This is the predicted rating\n",
    "# 4. Backward pass\n",
    "#      We compare the predicted rating with the label rating using MSE.\n",
    "#      Update the user and item embedding tables accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model: we create user and item embedding tables for each unique user and unique item; \n",
    "# forward pass by element-wise multiplication and summation\n",
    "\n",
    "class MatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        # create user embeddings: each user is a tensor of length n_factors.\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors)\n",
    "        # create item embeddings: each item is a tensor of length n_factors\n",
    "        self.item_factors = torch.nn.Embedding(n_items, n_factors)\n",
    "        self.user_factors.weight.data.uniform_(0, 0.05)\n",
    "        self.item_factors.weight.data.uniform_(0, 0.05)\n",
    "\n",
    "    def forward(self, data):\n",
    "        users, items = data[:,0], data[:,1]\n",
    "        return (self.user_factors(users) * self.item_factors(items)).sum(1)\n",
    "\n",
    "    def predict(self, user, item):\n",
    "        return (self.user_factors(user) * self.item_factors(item)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need this to load the ratings pandas df into torch tensors. \n",
    "\n",
    "class Loader(Dataset):\n",
    "    def __init__(self, ratings_df):\n",
    "        self.ratings = ratings_df.copy()\n",
    "        users = ratings_df.userId.unique()\n",
    "        movies = ratings_df.movieId.unique()\n",
    "\n",
    "        self.userid2idx = {o:i for i,o in enumerate(users)}\n",
    "        self.movieid2idx = {o:i for i,o in enumerate(movies)}\n",
    "\n",
    "        self.idx2userid = {i:o for o,i in self.userid2idx.items()}\n",
    "        self.idx2movieid = {i:o for o,i in self.movieid2idx.items()}\n",
    "\n",
    "        self.ratings.movieId = ratings_df.movieId.apply(lambda x: self.movieid2idx[x])\n",
    "        self.ratings.userId = ratings_df.userId.apply(lambda x: self.userid2idx[x])\n",
    "\n",
    "        # userid and corresponding movieid that they rated\n",
    "        self.x = self.ratings.drop(['rating', 'timestamp'], axis=1).values\n",
    "        # rating\n",
    "        self.y = self.ratings['rating'].values\n",
    "        self.x, self.y = torch.tensor(self.x), torch.tensor(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_dev(train_df, dev_df, sgd_param):\n",
    "  \n",
    "  n_users = ratings_df['userId'].nunique()\n",
    "  n_items = ratings_df['movieId'].nunique()\n",
    "\n",
    "  model = MatrixFactorization(n_users, n_items, n_factors=sgd_param['n_factors'])\n",
    "  optimizer = optim.SGD(model.parameters(), lr=sgd_param['lr'])\n",
    "\n",
    "  cuda = torch.cuda.is_available()\n",
    "\n",
    "  if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "  loss_fn = nn.MSELoss()\n",
    "  train_set = Loader(train_df)\n",
    "  # This splits dataset into batches\n",
    "  train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for epoch in range(sgd_param['epochs']):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        # calculate loss\n",
    "        loss = loss_fn(output, target.float())\n",
    "        # calculate gradient\n",
    "        loss.backward()\n",
    "        # update parameters (embeddings in this case)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "  dev_set = Loader(dev_df)\n",
    "  dev_loader = DataLoader(dev_set, batch_size=128, shuffle=False)\n",
    "  # model to eval mode\n",
    "  model.eval()\n",
    "  dev_preds, dev_targets = [], []\n",
    "  with torch.no_grad():\n",
    "      for data, target in dev_loader:\n",
    "          data, target = data.cuda(), target.cuda()\n",
    "          output = model(data)\n",
    "          # append results\n",
    "          dev_preds.extend(output.cpu().numpy())\n",
    "          dev_targets.extend(target.cpu().numpy())\n",
    "\n",
    "  mae = mean_absolute_error(dev_targets, dev_preds)\n",
    "\n",
    "  print(f\"dev_mae for {sgd_param['n_factors']} factors, {sgd_param['lr']} lr, {sgd_param['epochs']} epochs: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_f in [10, 20, 50, 100]:\n",
    "  for n_ep in [10, 50, 100]:\n",
    "    for lr in [0.001, 0.01, 0.05]:\n",
    "      train_eval_dev(train_df, dev_df, {'n_factors': n_f, 'epochs': n_ep, 'lr': lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "\n",
    "# dev_mae for 10 factors, 0.001 lr, 10 epochs: 3.4900412809292685\n",
    "# dev_mae for 10 factors, 0.01 lr, 10 epochs: 3.480439918096354\n",
    "# dev_mae for 10 factors, 0.05 lr, 10 epochs: 3.028467126331669\n",
    "# dev_mae for 10 factors, 0.001 lr, 50 epochs: 3.486756268621376\n",
    "# dev_mae for 10 factors, 0.01 lr, 50 epochs: 3.026092983281317\n",
    "# dev_mae for 10 factors, 0.05 lr, 50 epochs: 1.3760415739293925\n",
    "# dev_mae for 10 factors, 0.001 lr, 100 epochs: 3.480752333602614\n",
    "# dev_mae for 10 factors, 0.01 lr, 100 epochs: 2.1650907983948624\n",
    "# dev_mae for 10 factors, 0.05 lr, 100 epochs: 1.1044064729262975\n",
    "# dev_mae for 20 factors, 0.001 lr, 10 epochs: 3.483121689989763\n",
    "# dev_mae for 20 factors, 0.01 lr, 10 epochs: 3.4644898156541255\n",
    "# dev_mae for 20 factors, 0.05 lr, 10 epochs: 2.8658078690881816\n",
    "# dev_mae for 20 factors, 0.001 lr, 50 epochs: 3.476532301745354\n",
    "# dev_mae for 20 factors, 0.01 lr, 50 epochs: 2.869004440047278\n",
    "# dev_mae for 20 factors, 0.05 lr, 50 epochs: 1.3476828530524387\n",
    "# dev_mae for 20 factors, 0.001 lr, 100 epochs: 3.4646104475111694\n",
    "# dev_mae for 20 factors, 0.01 lr, 100 epochs: 2.0680492953973366\n",
    "# dev_mae for 20 factors, 0.05 lr, 100 epochs: 1.0965616834833602\n",
    "# dev_mae for 50 factors, 0.001 lr, 10 epochs: 3.4622471793977434\n",
    "# dev_mae for 50 factors, 0.01 lr, 10 epochs: 3.4174553854509817\n",
    "# dev_mae for 50 factors, 0.05 lr, 10 epochs: 2.6361976552181994\n",
    "# dev_mae for 50 factors, 0.001 lr, 50 epochs: 3.445969564824434\n",
    "# dev_mae for 50 factors, 0.01 lr, 50 epochs: 2.6354201712233962\n",
    "# dev_mae for 50 factors, 0.05 lr, 50 epochs: 1.3040099250541246\n",
    "# dev_mae for 50 factors, 0.001 lr, 100 epochs: 3.4169696260114346\n",
    "# dev_mae for 50 factors, 0.01 lr, 100 epochs: 1.9297473189003211\n",
    "# dev_mae for 50 factors, 0.05 lr, 100 epochs: 1.0835130303438045\n",
    "# dev_mae for 100 factors, 0.001 lr, 10 epochs: 3.4275661815249725\n",
    "# dev_mae for 100 factors, 0.01 lr, 10 epochs: 3.3410197442210903\n",
    "# dev_mae for 100 factors, 0.05 lr, 10 epochs: 2.438167753674767\n",
    "# dev_mae for 100 factors, 0.001 lr, 50 epochs: 3.396717071175329\n",
    "# dev_mae for 100 factors, 0.01 lr, 50 epochs: 2.440149415787094\n",
    "# dev_mae for 100 factors, 0.05 lr, 50 epochs: 1.2641956796488487\n",
    "# dev_mae for 100 factors, 0.001 lr, 100 epochs: 3.341480087737653\n",
    "# dev_mae for 100 factors, 0.01 lr, 100 epochs: 1.8127504039392\n",
    "# dev_mae for 100 factors, 0.05 lr, 100 epochs: 1.0718658247723067 <- Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = ratings_df['userId'].nunique()\n",
    "n_items = ratings_df['movieId'].nunique()\n",
    "\n",
    "model = MatrixFactorization(n_users, n_items, n_factors=100)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "train_set = Loader(train_df)\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = (d.cuda() for d in data), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "test_set = Loader(test_df)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "test_preds, test_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = (d.cuda() for d in data), target.cuda()\n",
    "        output = model(data)\n",
    "        test_preds.extend(output.cpu().numpy())\n",
    "        test_targets.extend(target.cpu().numpy())\n",
    "\n",
    "mae = mean_absolute_error(test_targets, test_preds)\n",
    "mse = mean_squared_error(test_targets, test_preds)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results (Trained using lr=0.05, num_epochs=100, num_factors=100)\n",
    "\n",
    "# Epoch 1, Loss: 12.657633456630982\n",
    "# Epoch 2, Loss: 11.971283441004546\n",
    "# Epoch 3, Loss: 11.017647568730341\n",
    "# Epoch 4, Loss: 9.792050778001979\n",
    "# Epoch 5, Loss: 8.440822230732959\n",
    "# Epoch 6, Loss: 7.183058524477309\n",
    "# Epoch 7, Loss: 6.155719561853271\n",
    "# Epoch 8, Loss: 5.361280183861221\n",
    "# Epoch 9, Loss: 4.741093813509181\n",
    "# Epoch 10, Loss: 4.246194690897845\n",
    "# Epoch 11, Loss: 3.8416456921377042\n",
    "# Epoch 12, Loss: 3.505483899427497\n",
    "# Epoch 13, Loss: 3.2230168663073275\n",
    "# Epoch 14, Loss: 2.9834001491899076\n",
    "# Epoch 15, Loss: 2.778348690141802\n",
    "# Epoch 16, Loss: 2.6011971783810766\n",
    "# Epoch 17, Loss: 2.4458745169466822\n",
    "# Epoch 18, Loss: 2.3102050827465197\n",
    "# Epoch 19, Loss: 2.1896196316549745\n",
    "# Epoch 20, Loss: 2.08339984131896\n",
    "# Epoch 21, Loss: 1.9878152844266614\n",
    "# Epoch 22, Loss: 1.9015246370564336\n",
    "# Epoch 23, Loss: 1.825072992755019\n",
    "# Epoch 24, Loss: 1.7540460373612419\n",
    "# Epoch 25, Loss: 1.6904544553894927\n",
    "# Epoch 26, Loss: 1.6323633815931238\n",
    "# Epoch 27, Loss: 1.5781546265318773\n",
    "# Epoch 28, Loss: 1.529779164687447\n",
    "# Epoch 29, Loss: 1.483659920701082\n",
    "# Epoch 30, Loss: 1.4420938186239505\n",
    "# Epoch 31, Loss: 1.402962129941021\n",
    "# Epoch 32, Loss: 1.3663139747104782\n",
    "# Epoch 33, Loss: 1.333198330540588\n",
    "# Epoch 34, Loss: 1.300657980144024\n",
    "# Epoch 35, Loss: 1.271040684826996\n",
    "# Epoch 36, Loss: 1.2437281118354935\n",
    "# Epoch 37, Loss: 1.2176391514747038\n",
    "# Epoch 38, Loss: 1.1924792952511623\n",
    "# Epoch 39, Loss: 1.1692685065925985\n",
    "# Epoch 40, Loss: 1.1471906673649084\n",
    "# Epoch 41, Loss: 1.1271093536330306\n",
    "# Epoch 42, Loss: 1.1075532258204792\n",
    "# Epoch 43, Loss: 1.0887491647963938\n",
    "# Epoch 44, Loss: 1.0709219538215278\n",
    "# Epoch 45, Loss: 1.0545896566000537\n",
    "# Epoch 46, Loss: 1.0384291540021482\n",
    "# Epoch 47, Loss: 1.0232921919744948\n",
    "# Epoch 48, Loss: 1.0088078245736551\n",
    "# Epoch 49, Loss: 0.9950371460206267\n",
    "# Epoch 50, Loss: 0.9821465224891469\n",
    "# Epoch 51, Loss: 0.9693458156957142\n",
    "# Epoch 52, Loss: 0.9573148699558299\n",
    "# Epoch 53, Loss: 0.9458891057233879\n",
    "# Epoch 54, Loss: 0.9350817700227102\n",
    "# Epoch 55, Loss: 0.9240121945090916\n",
    "# Epoch 56, Loss: 0.9145387292340181\n",
    "# Epoch 57, Loss: 0.9041371773118558\n",
    "# Epoch 58, Loss: 0.8947801643955535\n",
    "# Epoch 59, Loss: 0.8860369059054748\n",
    "# Epoch 60, Loss: 0.8772839566935664\n",
    "# Epoch 61, Loss: 0.8685462648669878\n",
    "# Epoch 62, Loss: 0.8604699603241422\n",
    "# Epoch 63, Loss: 0.8526288756857747\n",
    "# Epoch 64, Loss: 0.8452572154178135\n",
    "# Epoch 65, Loss: 0.8381637550782466\n",
    "# Epoch 66, Loss: 0.8308001554746559\n",
    "# Epoch 67, Loss: 0.8246787493319615\n",
    "# Epoch 68, Loss: 0.8175459005802438\n",
    "# Epoch 69, Loss: 0.8111940079193184\n",
    "# Epoch 70, Loss: 0.8048043349298878\n",
    "# Epoch 71, Loss: 0.7990637609492177\n",
    "# Epoch 72, Loss: 0.7934688431197319\n",
    "# Epoch 73, Loss: 0.7872064919143483\n",
    "# Epoch 74, Loss: 0.7821175379921561\n",
    "# Epoch 75, Loss: 0.7767087594754454\n",
    "# Epoch 76, Loss: 0.7714976891875267\n",
    "# Epoch 77, Loss: 0.7664382398344468\n",
    "# Epoch 78, Loss: 0.7614117902366148\n",
    "# Epoch 79, Loss: 0.7570925200546997\n",
    "# Epoch 80, Loss: 0.7517694356864777\n",
    "# Epoch 81, Loss: 0.747075438823389\n",
    "# Epoch 82, Loss: 0.7431324055229408\n",
    "# Epoch 83, Loss: 0.7385387542671051\n",
    "# Epoch 84, Loss: 0.734258241681517\n",
    "# Epoch 85, Loss: 0.7298031257654446\n",
    "# Epoch 86, Loss: 0.725728320794693\n",
    "# Epoch 87, Loss: 0.7220588968391868\n",
    "# Epoch 88, Loss: 0.718305023090131\n",
    "# Epoch 89, Loss: 0.7143210173193095\n",
    "# Epoch 90, Loss: 0.7102118381231591\n",
    "# Epoch 91, Loss: 0.7065887288122937\n",
    "# Epoch 92, Loss: 0.7030698830882708\n",
    "# Epoch 93, Loss: 0.6994529672614906\n",
    "# Epoch 94, Loss: 0.6958090996612674\n",
    "# Epoch 95, Loss: 0.6922917447552301\n",
    "# Epoch 96, Loss: 0.688913749233968\n",
    "# Epoch 97, Loss: 0.6858701647083828\n",
    "# Epoch 98, Loss: 0.6822459414277388\n",
    "# Epoch 99, Loss: 0.6793227819860845\n",
    "# Epoch 100, Loss: 0.6761781242878541\n",
    "# Mean Absolute Error: 1.0772188565851786\n",
    "# Mean Squared Error: 1.801882512378324"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
